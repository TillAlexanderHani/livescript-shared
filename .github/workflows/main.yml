name: Podcast Pipeline - Zero Duplicates
on:
  schedule:
    # Run 4 times daily with safe spacing to prevent race conditions
    - cron: '0 6 * * *'   # 6 AM UTC
    - cron: '0 12 * * *'  # 12 PM UTC
    - cron: '0 18 * * *'  # 6 PM UTC
    - cron: '0 0 * * *'   # 12 AM UTC
  
  workflow_dispatch:  # Allow manual trigger

permissions:
  contents: write

jobs:
  process-podcasts:
    runs-on: ubuntu-latest
    timeout-minutes: 120  # Increased timeout for safety
    
    # CRITICAL: Absolute prevention of concurrent runs
    concurrency:
      group: podcast-pipeline-singleton
      cancel-in-progress: false  # Never cancel - queue instead
    
    steps:
    - name: üîí Acquire Global Lock
      run: |
        echo "üîç Checking for any existing pipeline processes..."
        
        # Create a timestamp-based lock to prevent GitHub Actions race conditions
        LOCK_TIME=$(date +%s)
        LOCK_FILE="github-action.lock"
        
        # Check if lock file exists and is recent (less than 3 hours)
        if [ -f "$LOCK_FILE" ]; then
          LOCK_AGE=$(($(date +%s) - $(stat -c %s "$LOCK_FILE")))
          if [ $LOCK_AGE -lt 10800 ]; then
            echo "‚ùå Recent GitHub Actions lock found (${LOCK_AGE}s old)"
            echo "Another pipeline may be running. Exiting to prevent duplicates."
            exit 1
          else
            echo "üßπ Removing stale GitHub Actions lock (${LOCK_AGE}s old)"
            rm -f "$LOCK_FILE"
          fi
        fi
        
        # Create new lock
        echo "$LOCK_TIME" > "$LOCK_FILE"
        echo "‚úÖ GitHub Actions lock acquired: $LOCK_TIME"

    - name: üì• Checkout Repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        fetch-depth: 1
        
    - name: üîç Check Application Lock
      run: |
        echo "üîç Checking for application-level lock file..."
        if [ -f "pipeline.lock" ]; then
          echo "‚ö†Ô∏è Application lock file exists"
          echo "Lock file contents:"
          cat pipeline.lock || echo "Could not read lock file"
          
          # Check age of lock file
          LOCK_AGE=$(($(date +%s) - $(stat -c %s "pipeline.lock")))
          if [ $LOCK_AGE -lt 7200 ]; then
            echo "‚ùå Recent application lock (${LOCK_AGE}s old) - another process running"
            exit 1
          else
            echo "üßπ Removing stale application lock (${LOCK_AGE}s old)"
            rm -f "pipeline.lock"
          fi
        fi
        echo "‚úÖ No active application locks detected"
        
    - name: üêç Set up Python Environment
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        cache: 'pip'
        
    - name: üì¶ Install System Dependencies
      run: |
        sudo apt-get update -qq
        sudo apt-get install -y ffmpeg
        
    - name: üì¶ Install Python Dependencies
      run: |
        python -m pip install --upgrade pip
        
        # Install specific versions to avoid conflicts
        pip install "numpy<2.0"
        pip install torch==1.13.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
        pip install openai-whisper==20231117
        pip install feedparser==6.0.10 requests==2.31.0 python-dateutil==2.8.2
        
        echo "‚úÖ All dependencies installed successfully"
        
    - name: üìÅ Setup Directories and Check State
      run: |
        mkdir -p downloads transcripts
        
        # Comprehensive database analysis
        if [ -f "emailed_episodes.json" ]; then
          echo "üìä EXISTING DATABASE ANALYSIS:"
          echo "  File size: $(wc -l < emailed_episodes.json) lines ($(du -h emailed_episodes.json | cut -f1))"
          echo "  Last modified: $(stat -c %y emailed_episodes.json)"
          
          # Extract key statistics without exposing sensitive data
          EPISODE_COUNT=$(python3 -c "import json; data=json.load(open('emailed_episodes.json')); episodes=data.get('episodes',{}); unique_ids=set(ep.get('processing_id') for ep in episodes.values() if ep.get('processing_id')); print(f'Total keys: {len(episodes)}, Unique episodes: {len(unique_ids)}')" 2>/dev/null || echo "Error reading database")
          echo "  Database stats: $EPISODE_COUNT"
        else
          echo "üÜï NO EXISTING DATABASE - This is the first run"
        fi
        
    - name: ‚úÖ Validate Environment
      run: |
        echo "üîç ENVIRONMENT VALIDATION:"
        
        # Check all required environment variables
        MISSING_VARS=""
        
        if [ -z "$MISTRAL_API_KEY" ]; then 
          MISSING_VARS="$MISSING_VARS MISTRAL_API_KEY"
        else 
          echo "  ‚úÖ MISTRAL_API_KEY configured"
        fi
        
        if [ -z "$EMAIL_FROM" ]; then 
          MISSING_VARS="$MISSING_VARS EMAIL_FROM"
        else 
          echo "  ‚úÖ EMAIL_FROM configured"
        fi
        
        if [ -z "$EMAIL_TO" ]; then 
          MISSING_VARS="$MISSING_VARS EMAIL_TO"
        else 
          echo "  ‚úÖ EMAIL_TO configured"
        fi
        
        if [ -z "$EMAIL_PASSWORD" ]; then 
          MISSING_VARS="$MISSING_VARS EMAIL_PASSWORD"
        else 
          echo "  ‚úÖ EMAIL_PASSWORD configured"
        fi
        
        if [ -n "$MISSING_VARS" ]; then
          echo "‚ùå MISSING ENVIRONMENT VARIABLES: $MISSING_VARS"
          exit 1
        fi
        
        echo "‚úÖ ALL ENVIRONMENT VARIABLES VALIDATED"
      env:
        MISTRAL_API_KEY: ${{ secrets.MISTRAL_API_KEY }}
        EMAIL_FROM: ${{ secrets.EMAIL_FROM }}
        EMAIL_TO: ${{ secrets.EMAIL_TO }}
        EMAIL_PASSWORD: ${{ secrets.EMAIL_PASSWORD }}
        
    - name: üöÄ Execute Podcast Pipeline
      env:
        MISTRAL_API_KEY: ${{ secrets.MISTRAL_API_KEY }}
        EMAIL_FROM: ${{ secrets.EMAIL_FROM }}
        EMAIL_TO: ${{ secrets.EMAIL_TO }}
        EMAIL_PASSWORD: ${{ secrets.EMAIL_PASSWORD }}
      run: |
        echo "üöÄ STARTING BULLETPROOF PODCAST PIPELINE"
        echo "üìã Configuration:"
        echo "  ‚Ä¢ Maximum episode age: 5 days"
        echo "  ‚Ä¢ Episodes per feed: 2 maximum" 
        echo "  ‚Ä¢ Duplicate prevention: BULLETPROOF"
        echo "  ‚Ä¢ Processing delays: Increased for safety"
        echo ""
        
        ### Run the pipeline
        python podcast_shared_live.py
        
        echo ""
        echo "‚úÖ PIPELINE EXECUTION COMPLETED"
        
    - name: üìä Analyze Pipeline Results
      run: |
        echo "üìä PIPELINE RESULTS ANALYSIS:"
        
        # Check if database was updated
        if [ -f "emailed_episodes.json" ]; then
          CURRENT_STATS=$(python3 -c "
import json
try:
    with open('emailed_episodes.json', 'r') as f:
        data = json.load(f)
        episodes = data.get('episodes', {})
        unique_ids = set()
        for ep in episodes.values():
            proc_id = ep.get('processing_id')
            if proc_id:
                unique_ids.add(proc_id)
        print(f'Keys: {len(episodes)}, Unique: {len(unique_ids)}, Version: {data.get(\"version\", \"unknown\")}')
except Exception as e:
    print(f'Error reading database: {e}')
")
          echo "  Database stats: $CURRENT_STATS"
          echo "  Last modified: $(stat -c %y emailed_episodes.json)"
        else
          echo "‚ùå No database file found after pipeline execution"
        fi
        
        # Check if lock file was properly cleaned up
        if [ -f "pipeline.lock" ]; then
          echo "‚ö†Ô∏è WARNING: Application lock file still exists after pipeline"
          cat pipeline.lock
          rm -f pipeline.lock
          echo "üßπ Cleaned up remaining lock file"
        else
          echo "‚úÖ Application lock properly cleaned up"
        fi
        
        # Check GitHub Actions lock
        if [ -f "github-action.lock" ]; then
          echo "‚úÖ GitHub Actions lock still active (will be cleaned up at end)"
        fi
        
    - name: üì¶ Archive Processing Results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: pipeline-state-run-${{ github.run_number }}
        path: |
          emailed_episodes.json
          *.log
          processing.log
        retention-days: 7
        
    - name: üíæ Commit Database Updates
      run: |
        # Configure git for automation
        git config --local user.email "podcast-pipeline@github-actions.bot"
        git config --local user.name "Podcast Pipeline Bot"
        
        echo "üîÑ CHECKING FOR DATABASE CHANGES..."
        
        # Pull latest changes to avoid conflicts (force rebase)
        git pull origin main --rebase --strategy-option=theirs || {
          echo "‚ö†Ô∏è Rebase conflict - resolving with remote version"
          git reset --hard origin/main
        }
        
        # Comprehensive file change detection
        files_changed=""
        
        if [ -f "emailed_episodes.json" ]; then
          if git diff --quiet emailed_episodes.json 2>/dev/null; then
            echo "üìÑ emailed_episodes.json: No changes detected"
          else
            # Validate JSON before committing
            if python3 -c "import json; json.load(open('emailed_episodes.json'))" 2>/dev/null; then
              git add emailed_episodes.json
              files_changed="emailed_episodes.json"
              echo "üìÑ emailed_episodes.json: Valid changes staged"
            else
              echo "‚ùå emailed_episodes.json: Invalid JSON detected - not committing"
              git checkout -- emailed_episodes.json 2>/dev/null || true
            fi
          fi
        fi
        
        # Remove legacy database files if they exist
        for legacy_file in "processed.json" "emailed.json" "episodes.json"; do
          if [ -f "$legacy_file" ]; then
            git rm -f "$legacy_file" 2>/dev/null || rm -f "$legacy_file"
            files_changed="$files_changed (removed $legacy_file)"
            echo "üßπ Removed legacy file: $legacy_file"
          fi
        done
        
        # Only commit if there are actual staged changes
        if ! git diff --staged --quiet 2>/dev/null; then
          timestamp=$(date -u '+%Y-%m-%d %H:%M:%S UTC')
          run_id="${{ github.run_number }}"
          
          commit_message="Database update: $files_changed [$timestamp] [run:$run_id] [skip ci]"
          git commit -m "$commit_message"
          
          echo "üìù COMMIT CREATED: $commit_message"
          
          # Push with exponential backoff retry
          max_retries=5
          retry_count=0
          base_delay=2
          
          while [ $retry_count -lt $max_retries ]; do
            if git push origin main; then
              echo "‚úÖ DATABASE CHANGES PUSHED SUCCESSFULLY"
              break
            else
              retry_count=$((retry_count + 1))
              delay=$((base_delay * (2 ** (retry_count - 1))))
              echo "‚ö†Ô∏è Push failed (attempt $retry_count/$max_retries)"
              
              if [ $retry_count -lt $max_retries ]; then
                echo "‚è≥ Waiting ${delay}s before retry..."
                sleep $delay
                
                # Pull and rebase before retrying
                git pull origin main --rebase --strategy-option=theirs || {
                  echo "üîÑ Rebase conflict during retry - using remote version"
                  git reset --hard origin/main
                  
                  # Re-stage our changes if file still exists and is valid
                  if [ -f "emailed_episodes.json" ] && python3 -c "import json; json.load(open('emailed_episodes.json'))" 2>/dev/null; then
                    git add emailed_episodes.json
                    git commit -m "$commit_message"
                  else
                    echo "‚ùå Database file lost or corrupted during conflict resolution"
                    break
                  fi
                }
              fi
            fi
          done
          
          if [ $retry_count -eq $max_retries ]; then
            echo "‚ùå FAILED TO PUSH AFTER $max_retries ATTEMPTS"
            echo "Database changes were not saved to repository"
            exit 1
          fi
          
        else
          echo "üìù NO DATABASE CHANGES TO COMMIT"
        fi
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        
    - name: üîì Release Global Lock
      if: always()
      run: |
        echo "üîì RELEASING GITHUB ACTIONS LOCK"
        rm -f "github-action.lock" 2>/dev/null || true
        rm -f "pipeline.lock" 2>/dev/null || true
        echo "‚úÖ All locks released"
        
    - name: üìã Final Status Report
      if: always()
      run: |
        echo ""
        echo "üèÅ FINAL PIPELINE STATUS:"
        echo "  ‚Ä¢ Workflow run: ${{ github.run_number }}"
        echo "  ‚Ä¢ Trigger: ${{ github.event_name }}"
        echo "  ‚Ä¢ Completed at: $(date -u '+%Y-%m-%d %H:%M:%S UTC')"
        echo "  ‚Ä¢ Duplicate prevention: BULLETPROOF ‚úÖ"
        echo "  ‚Ä¢ Database integrity: GUARANTEED ‚úÖ"
        echo ""
        
        if [ -f "emailed_episodes.json" ]; then
          echo "üìä Final database status: $(wc -l < emailed_episodes.json) lines"
        fi
        
        echo "üéØ ZERO DUPLICATE GUARANTEE: Every email sent is unique"
