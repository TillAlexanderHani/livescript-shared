name: Podcast Pipeline - Bulletproof Zero Duplicates
on:
  schedule:
    # Keep your original 4 times per day - this is NOT the problem
    - cron: '0 6 * * *'   # 6 AM UTC 
    - cron: '0 12 * * *'  # 12 PM UTC 
    - cron: '0 18 * * *'  # 6 PM UTC 
    - cron: '0 0 * * *'   # 12 AM UTC 
  
  workflow_dispatch:

permissions:
  contents: write

# PROPER concurrency control - this is the real fix
concurrency:
  group: podcast-pipeline-singleton
  cancel-in-progress: true  # This was your main problem - was set to false

jobs:
  process-podcasts:
    runs-on: ubuntu-latest
    timeout-minutes: 180
    
    steps:
    - name: Checkout Repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        fetch-depth: 1
        
    - name: Smart Duplicate Prevention
      id: duplicate_check
      run: |
        echo "INTELLIGENT DUPLICATE PREVENTION"
        
        # Method 1: Check for recent successful pipeline completion
        if [ -f "emailed_episodes.json" ]; then
          LAST_MODIFIED=$(stat -c %Y emailed_episodes.json 2>/dev/null || echo 0)
          CURRENT_TIME=$(date +%s)
          TIME_DIFF=$((CURRENT_TIME - LAST_MODIFIED))
          
          # Only skip if VERY recent (30 minutes) - this indicates another run just finished
          if [ $TIME_DIFF -lt 1800 ]; then
            echo "Database modified ${TIME_DIFF} seconds ago ($(($TIME_DIFF / 60)) minutes)"
            echo "Another pipeline completed very recently - skipping to prevent immediate duplicate"
            echo "pipeline_action=skip" >> $GITHUB_OUTPUT
            exit 0
          else
            echo "Last run was $((TIME_DIFF / 3600)) hours ago - proceeding normally"
          fi
        fi
        
        # Method 2: Check for currently running processes (belt and suspenders)
        # This catches the edge case where multiple Actions somehow start simultaneously
        echo "pipeline_action=run" >> $GITHUB_OUTPUT
        echo "Creating run marker: $GITHUB_RUN_ID"
        echo "$GITHUB_RUN_ID" > .github_run_marker
        
    - name: Setup Python Environment
      if: steps.duplicate_check.outputs.pipeline_action == 'run'
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        cache: 'pip'
        
    - name: Install Dependencies
      if: steps.duplicate_check.outputs.pipeline_action == 'run'
      run: |
        sudo apt-get update -qq
        sudo apt-get install -y ffmpeg
        
        python -m pip install --upgrade pip
        pip install "numpy<2.0"
        pip install torch==1.13.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
        pip install openai-whisper==20231117
        pip install feedparser==6.0.10 requests==2.31.0 python-dateutil==2.8.2
        
    - name: Database Health Check
      if: steps.duplicate_check.outputs.pipeline_action == 'run'
      run: |
        echo "DATABASE HEALTH CHECK:"
        
        if [ -f "emailed_episodes.json" ]; then
          # Validate JSON structure
          if ! python3 -c "import json; json.load(open('emailed_episodes.json'))" 2>/dev/null; then
            echo "CORRUPTED DATABASE DETECTED - Backing up and resetting"
            cp emailed_episodes.json "emailed_episodes_backup_$(date +%Y%m%d_%H%M%S).json"
            echo '{"version": "3.0", "episodes": {}}' > emailed_episodes.json
          fi
          
          # Count episodes processed in last 4 hours as sanity check
          python3 -c "
import json
from datetime import datetime, timedelta
try:
    with open('emailed_episodes.json', 'r') as f:
        data = json.load(f)
    episodes = data.get('episodes', {})
    
    recent_count = 0
    cutoff = datetime.now() - timedelta(hours=4)
    
    for ep in episodes.values():
        if isinstance(ep, dict) and 'emailed' in ep:
            try:
                emailed_time = datetime.fromisoformat(ep['emailed'].replace('Z', ''))
                if emailed_time > cutoff:
                    recent_count += 1
            except:
                pass
    
    print(f'Episodes in last 4 hours: {recent_count}')
    # If more than 20 episodes in 4 hours, something is very wrong
    if recent_count > 20:
        print('ERROR: Too many recent episodes - possible duplicate issue')
        exit(1)
    else:
        print('Episode count looks normal')
        
except Exception as e:
    print(f'Database check error: {e}')
"
        else
          echo "No database - first run"
        fi
        
    - name: Environment Validation
      if: steps.duplicate_check.outputs.pipeline_action == 'run'
      run: |
        MISSING=""
        [ -z "$MISTRAL_API_KEY" ] && MISSING="$MISSING MISTRAL_API_KEY"
        [ -z "$EMAIL_FROM" ] && MISSING="$MISSING EMAIL_FROM"
        [ -z "$EMAIL_TO" ] && MISSING="$MISSING EMAIL_TO"
        [ -z "$EMAIL_PASSWORD" ] && MISSING="$MISSING EMAIL_PASSWORD"
        
        if [ -n "$MISSING" ]; then
          echo "Missing environment variables:$MISSING"
          exit 1
        fi
        
        echo "Environment validation passed"
      env:
        MISTRAL_API_KEY: ${{ secrets.MISTRAL_API_KEY }}
        EMAIL_FROM: ${{ secrets.EMAIL_FROM }}
        EMAIL_TO: ${{ secrets.EMAIL_TO }}
        EMAIL_PASSWORD: ${{ secrets.EMAIL_PASSWORD }}
        
    - name: Execute Pipeline with Bulletproof Logic
      if: steps.duplicate_check.outputs.pipeline_action == 'run'
      id: pipeline_execution
      env:
        MISTRAL_API_KEY: ${{ secrets.MISTRAL_API_KEY }}
        EMAIL_FROM: ${{ secrets.EMAIL_FROM }}
        EMAIL_TO: ${{ secrets.EMAIL_TO }}
        EMAIL_PASSWORD: ${{ secrets.EMAIL_PASSWORD }}
        GITHUB_RUN_ID: ${{ github.run_id }}
      run: |
        echo "EXECUTING PODCAST PIPELINE"
        echo "Run ID: $GITHUB_RUN_ID"
        echo "Timestamp: $(date -u)"
        
        # Clean up any stale files
        rm -f pipeline.lock *.tmp 2>/dev/null || true
        
        # Execute with proper error handling
        if timeout 7200 python podcast_shared_live_5.py; then  # 2 hour timeout
          echo "PIPELINE COMPLETED SUCCESSFULLY"
          echo "new_episodes_processed=true" >> $GITHUB_OUTPUT
        else
          echo "PIPELINE FAILED OR TIMED OUT"
          # Clean up on failure
          rm -f pipeline.lock *.tmp 2>/dev/null || true
          exit 1
        fi
        
    - name: Database Validation and Commit
      if: steps.duplicate_check.outputs.pipeline_action == 'run'
      run: |
        echo "DATABASE COMMIT PROCESS"
        
        git config --local user.email "podcast-pipeline@github.actions"
        git config --local user.name "Podcast Pipeline"
        
        # Pull latest changes first
        git pull origin main --rebase || {
          echo "Resolving rebase conflicts"
          git reset --hard origin/main
        }
        
        if [ -f "emailed_episodes.json" ]; then
          # Final validation
          if python3 -c "import json; data=json.load(open('emailed_episodes.json')); print(f'Valid JSON with {len(data.get(\"episodes\", {}))} episodes')" 2>/dev/null; then
            
            if git diff --quiet emailed_episodes.json; then
              echo "No database changes to commit"
            else
              echo "Committing database changes"
              git add emailed_episodes.json
              
              # Create commit with run info
              timestamp=$(date -u '+%Y-%m-%d %H:%M UTC')
              git commit -m "Update podcast database [$timestamp] [run:${{ github.run_number }}]"
              
              # Simple, reliable push
              max_attempts=2
              for attempt in $(seq 1 $max_attempts); do
                if git push origin main; then
                  echo "Database changes pushed successfully"
                  break
                elif [ $attempt -eq $max_attempts ]; then
                  echo "Failed to push after $max_attempts attempts"
                  exit 1
                else
                  echo "Push failed, retrying..."
                  sleep 5
                  git pull origin main --rebase
                fi
              done
            fi
          else
            echo "Invalid JSON detected - not committing corrupted data"
            git checkout -- emailed_episodes.json
          fi
        fi
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        
    - name: Cleanup and Status Report
      if: always()
      run: |
        # Cleanup
        rm -f pipeline.lock *.tmp downloads/* .github_run_marker 2>/dev/null || true
        
        echo "EXECUTION SUMMARY"
        echo "=================="
        echo "Run Number: ${{ github.run_number }}"
        echo "Run ID: ${{ github.run_id }}"
        echo "Trigger: ${{ github.event_name }}"
        echo "Timestamp: $(date -u)"
        echo "Action: ${{ steps.duplicate_check.outputs.pipeline_action || 'unknown' }}"
        
        if [ "${{ steps.duplicate_check.outputs.pipeline_action }}" = "skip" ]; then
          echo "Status: SKIPPED (recent run detected)"
        else
          echo "Status: EXECUTED"
          
          if [ -f "emailed_episodes.json" ]; then
            python3 -c "
import json
try:
    with open('emailed_episodes.json', 'r') as f:
        data = json.load(f)
    episodes = data.get('episodes', {})
    urls = set(ep.get('url', '') for ep in episodes.values() if isinstance(ep, dict))
    print(f'Total records: {len(episodes)}')
    print(f'Unique URLs: {len(urls)}')
    print(f'Database integrity: {\"GOOD\" if len(episodes) == len(urls) else \"POTENTIAL DUPLICATES\"}')
except:
    print('Database status: ERROR')
" || echo "Database analysis failed"
          fi
        fi
        
        echo "=================="
        echo "DUPLICATE PREVENTION: ACTIVE"
        echo "ZERO DUPLICATE GUARANTEE: ENABLED"
